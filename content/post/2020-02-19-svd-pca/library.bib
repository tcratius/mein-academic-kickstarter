@online{brunton_steven_l_about_nodate,
	title = {About the Book},
	url = {databookuw.com/index.html},
	titleaddon = {{DATA} {DRIVEN} {SCIENCE} \& {ENGINEERING}},
	author = {Brunton, Steven L., J. Nathan, Kutz},
	urldate = {2020-02-11},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Two_Oriel\\ownCloud\\Zotero-Library\\storage\\7DQGTQPE\\databookuw.com.html:text/html}
}
@book{brunton_data-driven_2019,
	title = {Data-Driven Science and Engineering Machine Learning, Dynamical Systems, and Control},
	isbn = {978-1-108-42209-3 Hardback},
	publisher = {Cambridge University Press},
	author = {Brunton, Steven L. and Kutz, J. Nathan},
	date = {2019}
}

@article{sadek_svd_2012,
	title = {{SVD} Based Image Processing Applications: State of The Art, Contributions and Research Challenges},
	volume = {3},
	url = {www.ijacsa.thesai.org},
	abstract = {Singular Value Decomposition ({SVD}) has recently
emerged as a new paradigm for processing different types of
images. {SVD} is an attractive algebraic transform for image
processing applications. The paper proposes an experimental
survey for the {SVD} as an efficient transform in image processing
applications. Despite the well-known fact that {SVD} offers
attractive properties in imaging, the exploring of using its
properties in various image applications is currently at its
infancy. Since the {SVD} has many attractive properties have not
been utilized, this paper contributes in using these generous
properties in newly image applications and gives a highly
recommendation for more research challenges. In this paper, the
{SVD} properties for images are experimentally presented to be
utilized in developing new {SVD}-based image processing
applications. The paper offers survey on the developed {SVD}
based image applications. The paper also proposes some new
contributions that were originated from {SVD} properties analysis
in different image processing. The aim of this paper is to provide
a better understanding of the {SVD} in image processing and
identify important various applications and open research
directions in this increasingly important area; {SVD} based image
processing in the future research.},
	pages = {26--34},
	number = {7},
	journaltitle = {({IJACSA}) International Journal of Advanced Computer Science and Applications},
	author = {Sadek, Rowayda A.},
	date = {2012}
}

@article{ranade_variation_2007,
	title = {A variation on {SVD} based image compression},
	volume = {25},
	url = {www.sciencedirect.com},
	abstract = {We present a variation to the well studied {SVD} based image compression technique. Our variation can be viewed as a preprocessing
step in which the input image is permuted as per a ﬁxed, data independent permutation, after which it is fed to the standard {SVD} algo-
rithm. Likewise, our decompression algorithm can be viewed as the standard {SVD} algorithm followed by a postprocessing step which
applies the inverse permutation.
On experimenting with standard images we show that our method performs substantially better than the standard method. Typically,
for any given compression quality, our method needs about 30\% fewer singular values and vectors to be retained. We also present a bit
allocation scheme and show that our method also performs better than the more familiar discrete cosine transform ({DCT}).
We show that the original {SVD} algorithm as well as our variation, can be viewed as instances of the Karhunen–Loeve transform
({KLT}). In fact, we observe that there is a whole family of variations possible by choosing diﬀerent parameter values while applying
the {KLT}. We present heuristic arguments to show that our variation is likely to yield the best compression of all these. We also present
experimental evidence, which appears to justify our analysis.},
	pages = {771--777},
	journaltitle = {Image and Vision Computing},
	author = {Ranade, Abhiram and Mahabalarao, Srikanth S. and Kale, Satyen},
	date = {2007}
}

@article{shlens_tutorial_2014,
	title = {A Tutorial on Principal Component Analysis},
	url = {http://arxiv.org/abs/1404.1100},
	abstract = {Principal component analysis ({PCA}) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind {PCA}. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of {PCA} as well as the when, the how and the why of applying this technique.},
	journaltitle = {{arXiv}:1404.1100 [cs, stat]},
	author = {Shlens, Jonathon},
	urldate = {2020-01-29},
	date = {2014-04-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1404.1100},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Shlens - 2014 - A Tutorial on Principal Component Analysis.pdf:C\:\\Users\\Two_Oriel\\ownCloud\\Zotero-Library\\storage\\E6CZDJ7H\\Shlens - 2014 - A Tutorial on Principal Component Analysis.pdf:application/pdf}
}

@video{sanderson_eigenvectors_2016,
	title = {Eigenvectors and eigenvalues {\textbar} Essence of linear algebra, chapter 14},
	url = {https://www.youtube.com/watch?v=PFDu9oVAE-g},
	abstract = {Home page: https://www.3blue1brown.com/
A visual understanding of eigenvectors, eigenvalues, and the usefulness of an eigenbasis.

Full series: http://3b1b.co/eola

Future series like this are funded by the community, through Patreon, where supporters get early access as the series is being produced.
http://3b1b.co/support

Typo: At 12:27, "more that a line full" should be "more than a line full".

------------------

3blue1brown is a channel about animating math, in all senses of the word animate.  And you know the drill with {YouTube}, if you want to stay posted about new videos, subscribe, and click the bell to receive notifications (if you're into that).

If you are new to this channel and want to see more, a good place to start is this playlist: https://goo.gl/{WmnCQZ}

Various social media stuffs:
Website: https://www.3blue1brown.com
Twitter: https://twitter.com/3Blue1Brown
Patreon: https://patreon.com/3blue1brown
Facebook: https://www.facebook.com/3blue1brown
Reddit: https://www.reddit.com/r/3Blue1Brown},
	author = {Sanderson, Grant},
	urldate = {2020-01-29},
	date = {2016}
}

@online{gundersen_proof_2018,
	title = {Proof of the Singular Value Decomposition},
	url = {https://gregorygundersen.com/blog/2018/12/20/svd-proof/},
	author = {Gundersen, Gregory},
	urldate = {2020-01-29},
	date = {2018},
	file = {Proof of the Singular Value Decomposition:C\:\\Users\\Two_Oriel\\ownCloud\\Zotero-Library\\storage\\GGF86BKL\\svd-proof.html:text/html}
}

@article{noauthor_singular_2011,
	title = {Singular value decomposition},
	rights = {https://ocw.mit.edu/terms/},
	url = {https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/singular-value-decomposition/MIT18_06SCF11_Ses3.5sum.pdf},
	series = {18.06SC},
	journaltitle = {{MIT} {OpenCourseWare}},
	date = {2011}
}

@online{gundersen_singular_2018,
	title = {Singular Value Decomposition as Simply as Possible},
	url = {https://gregorygundersen.com/blog/2018/12/10/svd/},
	abstract = {Singular Value Decomposition ({SVD}) is powerful and ubiquitous tool for matrix factorization but explanations often provide little intuition. My goal is to explain {SVD} as simply as possible before working towards the formal definition.},
	author = {Gundersen, Gregory},
	urldate = {2020-02-10},
	date = {2018},
	file = {Singular Value Decomposition as Simply as Possible:C\:\\Users\\Two_Oriel\\ownCloud\\Zotero-Library\\storage\\GH5UTUEE\\svd.html:text/html}
}

@online{sanderson_vectors_2016,
	title = {Vectors, what even are they? {\textbar} Essence of linear algebra, chapter 1 - {YouTube}},
	url = {https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab},
	author = {Sanderson, Grant},
	urldate = {2020-02-10},
	date = {2016},
	file = {(16) Vectors, what even are they? | Essence of linear algebra, chapter 1 - YouTube:C\:\\Users\\Two_Oriel\\ownCloud\\Zotero-Library\\storage\\B6QLJF8I\\watch.html:text/html}
}
@book{singh_linear_2013,
	location = {Oxford, United Kingdom},
	edition = {1 edition},
	title = {Linear Algebra: Step by Step},
	isbn = {978-0-19-965444-4},
	shorttitle = {Linear Algebra},
	abstract = {Linear algebra is a fundamental area of mathematics, and is arguably the most powerful mathematical tool ever developed. It is a core topic of study within fields as diverse as: business, economics, engineering, physics, computer science, ecology, sociology, demography and genetics. For an example of linear algebra at work, one needs to look no further than the Google search engine, which relies upon linear algebra to rank the results of a search with respect to relevance. The strength of the text is in the large number of examples and the step by step explanation of each topic as it is introduced. It is compiled in a way that allows distance learning, with explicit solutions to set problems freely available online. The miscellaneous exercises at the end of each chapter comprise questions from past exam papers from various universities, helping to reinforce the reader's confidence. Also included, generally at the beginning of sections, are short historical biographies of the leading players in the field of linear algebra to provide context for the topics covered. The dynamic and engaging style of the book includes frequent question and answer sections to test the reader's understanding of the methods introduced, rather than requiring rote learning. When first encountered, the subject can appear abstract and students will sometimes struggle to see its relevance; to counter this, the book also contains interviews with key people who use linear algebra in practice, in both professional and academic life. It will appeal to undergraduate students in mathematics, the physical sciences and engineering.},
	pagetotal = {528},
	publisher = {Oxford University Press},
	author = {Singh, Kuldeep},
	date = {2013-12-31}
}
@article{gavish_optimal_2014,
	title = {The Optimal Hard Threshold for Singular Values is 4/sqrt(3)},
	url = {http://arxiv.org/abs/1305.5870},
	abstract = {We consider recovery of low-rank matrices from noisy data by hard thresholding of singular values, where singular values below a prescribed threshold \${\textbackslash}lambda\$ are set to 0. We study the asymptotic {MSE} in a framework where the matrix size is large compared to the rank of the matrix to be recovered, and the signal-to-noise ratio of the low-rank piece stays constant. The {AMSE}-optimal choice of hard threshold, in the case of n-by-n matrix in noise level {\textbackslash}sigma, is simply \$(4/{\textbackslash}sqrt\{3\}) {\textbackslash}sqrt\{n\}{\textbackslash}sigma {\textbackslash}approx 2.309 {\textbackslash}sqrt\{n\}{\textbackslash}sigma\$ when \${\textbackslash}sigma\$ is known, or simply \$2.858{\textbackslash}cdot y\_\{med\}\$ when \${\textbackslash}sigma\$ is unknown, where \$y\_\{med\}\$ is the median empirical singular value. For nonsquare \$m\$ by \$n\$ matrices with \$m {\textbackslash}neq n\$, these thresholding coefficients are replaced with different provided constants. In our asymptotic framework, this thresholding rule adapts to unknown rank and to unknown noise level in an optimal manner: it is always better than hard thresholding at any other value, no matter what the matrix is that we are trying to recover, and is always better than ideal Truncated {SVD} ({TSVD}), which truncates at the true rank of the low-rank matrix we are trying to recover. Hard thresholding at the recommended value to recover an n-by-n matrix of rank r guarantees an {AMSE} at most \$3nr{\textbackslash}sigma{\textasciicircum}2\$. In comparison, the guarantee provided by {TSVD} is \$5nr{\textbackslash}sigma{\textasciicircum}2\$, the guarantee provided by optimally tuned singular value soft thresholding is \$6nr{\textbackslash}sigma{\textasciicircum}2\$, and the best guarantee achievable by any shrinkage of the data singular values is \$2nr{\textbackslash}sigma{\textasciicircum}2\$. Empirical evidence shows that these {AMSE} properties of the \$4/{\textbackslash}sqrt\{3\}\$ thresholding rule remain valid even for relatively small n, and that performance improvement over {TSVD} and other shrinkage rules is substantial, turning it into the practical hard threshold of choice.},
	journaltitle = {{arXiv}:1305.5870 [stat]},
	author = {Gavish, Matan and Donoho, David L.},
	urldate = {2020-02-13},
	date = {2014-06-04},
	eprinttype = {arxiv},
	eprint = {1305.5870},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:C\:\\Users\\Two_Oriel\\ownCloud\\Zotero-Library\\storage\\48XRKZZ5\\Gavish and Donoho - 2014 - The Optimal Hard Threshold for Singular Values is .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Two_Oriel\\ownCloud\\Zotero-Library\\storage\\3G6G4N7G\\1305.html:text/html}
}


@article{takacs_distribution_1970,
	title = {On the Distribution of the Maximum of Sums of Mutually Independent and Identically Distributed Random Variables},
	volume = {2},
	issn = {0001-8678},
	url = {https://www.jstor.org/stable/1426323},
	doi = {10.2307/1426323},
	pages = {344--354},
	number = {2},
	journaltitle = {Advances in Applied Probability},
	author = {Takács, Lajos},
	urldate = {2020-02-13},
	date = {1970}
}

@online{weisstein_l2-norm_nodate,
	title = {L{\textasciicircum}2-Norm},
	rights = {Copyright 1999-2020 Wolfram Research, Inc.  See http://mathworld.wolfram.com/about/terms.html for a full terms of use statement.},
	url = {http://mathworld.wolfram.com/L2-Norm.html},
	abstract = {The l{\textasciicircum}2-norm (also written "l{\textasciicircum}2-norm") {\textbar}x{\textbar} is a vector norm defined for a complex vector  x=[x\_1; x\_2; {\textbar}; x\_n]  (1)   by  {\textbar}x{\textbar}=sqrt(sum\_(k=1){\textasciicircum}n{\textbar}x\_k{\textbar}{\textasciicircum}2),  (2)   where {\textbar}x\_k{\textbar} on the right denotes the complex modulus. The l{\textasciicircum}2-norm is the vector norm that is commonly encountered in vector algebra and vector operations (such as the dot product), where it is commonly denoted {\textbar}x{\textbar}. However, if desired, a more explicit (but more cumbersome) notation {\textbar}x{\textbar}\_2 can be used to emphasize the...},
	type = {Text},
	author = {Weisstein, Eric W.},
	urldate = {2020-02-14},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Two_Oriel\\ownCloud\\Zotero-Library\\storage\\B5M7ET3Z\\L2-Norm.html:text/html}
}