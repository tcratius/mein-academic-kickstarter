---
title: Unsupervised SVD thresholding
author: Conrad Thiele
date: '2020-07-31'
output:
  html_document:
    df_print: paged
csl: ieee-with-url.csl
slug: unsupervised-svd-thresholding
categories: ['Thresholding & Examples', 'SVD', 'Linear Algebra']
tags: ['SVD', 'Thresholding', 'Unsupervised']
subtitle: ''
summary: ''
authors: []
lastmod: '2020-07-31T14:46:22+10:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: library.bib
---



<div id="introduction---how-to-decide-on-a-cut-off-rank-in-noisy-data" class="section level3">
<h3>Introduction - How to decide on a cut-off rank in noisy data?</h3>
<p>In this article, the aim is to showcase the work of Gavish and Donoho <span class="citation">[1]</span> in finding a cut-off rank in data containing gaussian noise. Though a majority of the python code is taken and at times edited from the book by Bruton &amp; Kutz, <span class="citation">[2]</span> the inspiration behind this post comes directly from reading this book found at <a href="http://databookuw.com/">data driven science</a>. Alternatively, the Youtube series covering the book material is available at <a href="https://www.youtube.com/channel/UCm5mt-A4w61lknZ9lCsZtBw">Seve Brunton - Youtube channel</a>. Nevertheless, this would not be possible without the introduction to this topic by the Data Science team at <a href="www.jcu.edu.au">James Cook University</a>.</p>
</div>
<div id="unsupervised-approach---a-new-approach-for-thresholding." class="section level3">
<h3>Unsupervised approach - A new approach for thresholding.</h3>
<p>At the heart of this demonstration is white Gaussian noise. To understand what makes white noise troublesome comes down to the way independent and identically distributed dataset work. If there are two datasets, same size, both are symmetrical and independent and identically distributed matrices. The matrix <span class="math inline">\(X\)</span> in this scenario is the actual matrix, and the other matrix consist of <span class="math inline">\(Y= X + \sigma*{Z}\)</span> where <span class="math inline">\(\sigma*{Z}\)</span> is the standard deviation by the random numbers generated in the form of a normal distribution. Both of these matrices have the same probability that both samples <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> come from the same population mean <span class="math inline">\(\mu\)</span>, call it <span class="math inline">\(B\)</span>, such that <span class="math inline">\(x_{i},y_{i} \in B\)</span>, then both are within the same distribution along a “homogenous” Markov Chain <span class="citation">[3]</span>. The best way to visualise this is to think of all the dimensions being lay flat in a line, then it is easy to understand how the noise can become hidden in the population of this one dimensions array. Despite belonging to the same population, they do not have the same standard deviation <span class="math inline">\(Y\)</span> is now double the standard deviation of <span class="math inline">\(X\)</span> due to the artificial introduction of noise via the product of <span class="math inline">\(X + \sigma*{Z}\)</span>. Therefore, <span class="math inline">\(\sigma/2\)</span> can be used counters this effect. With that in mind, the Truncated SVD can be re-introduced to provide a possible solution to this tricky problem.</p>
<p>In 2014, Gavish and Donoho <span class="citation">[1]</span> began investigating a threshold method for dealing with Gaussian white embedded within the dataset. Gavish &amp; Donoho, 2014 found that past analyst has trailed multiple denoising techniques based around the Singular Value Hard Threshold (SVHT). The aim is to detect an elbow or dramatic drop off in the singular value plot, commonly known as scree plot, they would then set the rank cut-off to a value detect at this bulk edge. Any singular values below this mark is set to zero in what is known as the Truncated Singular Value Decomposition (TSVD) which can also be viewed in the <em>Code Snippet 2</em>. From there the duo used the Mean Squared Error to determine accuracy, where <span class="math inline">\(Y\)</span> with noise and <span class="math inline">\(X\)</span> is the matrix without noise. <span class="math display">\[\sum_{i,j}(Y_{i,j} -X_{i,j})^{2}\]</span>. The result was two algorithms that accurately estimates the necessary cut-off point in the Truncated Singular Values to achieve an optimal solution, and they depend on whether the matrix is square, nxn or rectangular, mxn and for the purpose of the equations below <span class="math inline">\(t\)</span> denotes the threshold.</p>
<p><span class="math display">\[ t = \frac{4}{\sqrt{3}}\sqrt{n}\sigma\]</span>
- In this case sigma <span class="math inline">\(\sigma\)</span> is the standard deviation and nxn matrix.</p>
<span class="math display">\[t = \lambda*(\beta)\cdot\sqrt{n}\sigma\]</span>
- <span class="math inline">\(\lambda\)</span> is the optimal hard threshold from <em>Table 1.</em><br />
- <span class="math inline">\(\beta = m/n\)</span> for mxn matrix.
<center>
<img src="images/Optimal-Hard-Threshold-Coef.jpg" />
</center>
<center>
<em>Table 1 - Optimal Hard Threshold Coefficients</em><br><br>
</center>
</div>
<div id="reintroducing-humans-best-friend." class="section level3">
<h3>Reintroducing humans best friend.</h3>
<p>In this section of code, the image of the dog is read, converted to grey scale, and displayed in <em>See Figure 1</em>.</p>
<pre class="python"><code>from matplotlib.image import imread
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import os
plt.rcParams[&#39;figure.figsize&#39;] = [16, 8]
plt.rcParams.update({&#39;font.size&#39;: 18})

A = imread(&#39;images/dog.jpg&#39;)
A = np.mean(A, -1); # Convert RGB to grayscale


img = plt.imshow(A)
img.set_cmap(&#39;gray&#39;)
plt.axis(&#39;off&#39;)
np.savetxt(&quot;test.csv&quot;, A[1:10], delimiter=&quot;,&quot;)
plt.savefig(&quot;images/grey_dog.png&quot;)
Asize = os.path.getsize(&quot;images/grey_dog.png&quot;)
plt.title(&quot;Original: Shape &quot; + str(A.shape) + &quot; &amp; Size &quot; + str(Asize/1000) + &quot;KB&quot;)
plt.show()</code></pre>
<img src="images/grey_dog.png" />
<center>
<em>Figure 1 - Grey scaled dog.</em><br><br>
</center>
</div>
<div id="bring-the-noise---guassian." class="section level3">
<h3>Bring the noise - Guassian.</h3>
<p>Next, the noise is created and added to the image using the standard deviation of <span class="math inline">\(X\)</span> multiplied by the randn function which computes a random normal distrubution based on the dimensions of X. The result is a dog with added Gaussian noise, <em>Figure 2</em>, however, note with the following table that the mean stays the same for both X and Y matrices, yet the standard deviation changes. As such, the Gaussian noise spread them evenly through the distribution only slightly left and right of the mean yet they are still within valid bounds of a sampled distribution, i.e. three standard deviations from the mean. Therefore, removing outliers by methods such as boxplot analysis would be extremely time-consuming as there are two thousand rows to analyse, which is possible with a batching algorithm, where is it useful or not is not tested here.</p>
<pre class="python"><code># Assign the grey scale image in A to the variable X then 
# get the std of X.
X = A
sigma = np.std(X)


# Add noise and matrix X, save the image and visualise the new image
Y = X + sigma*np.random.randn(*X.shape) 
img = plt.imshow(Y)
img.set_cmap(&#39;gray&#39;)
plt.axis(&#39;off&#39;)
plt.savefig(&quot;images/noisy_plus_dog.png&quot;)
plt.show()


# print general stats before and after adding noise.
print(&quot;Pre &amp; Post Noise - Mean and STD.&quot;)
print(&quot;-&quot;* 30)
print(&quot;Standard Deviation of X: {:0.2f}&quot;.format(np.std(X)))
print(&quot;Mean of X: {:18.2f}&quot;.format(np.mean(X)))
print(&quot;Standard deviation of Y: {:0.2f}&quot;.format(np.std(Y)))
print(&quot;Mean of Y: {:18.2f}&quot;.format(np.mean(Y)))</code></pre>
<img src="images/noisy_plus_dog.png" />
<center>
_Figure 2 - Image with sigam*Z noise._<br><br>
</center>
<table>
<thead>
<tr class="header">
<th>Mean and STD</th>
<th>Pre-Noise</th>
<th>Post-Noise</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>STD</td>
<td>0.27</td>
<td>0.38</td>
</tr>
<tr class="even">
<td>Mean</td>
<td>0.66</td>
<td>0.66</td>
</tr>
</tbody>
</table>
<center>
_Table 1 - Image with sigam*Z noise pre and post noise._<br><br>
</center>
<p>The following algorithms to compute the <span class="math inline">\(beta\)</span> and <span class="math inline">\(omega\)</span> values were taken from the “The Optimal Hard Threshold for Singular Values is <span class="math inline">\(4\sqrt3\)</span>”, yet the authors even stated that finding were not successful when <span class="math inline">\(beta &gt; 1\)</span>. As a result of experimentation the following logic was developed to keep the output within the range of zero and one, and it still maintained fairly decent results;</p>
<p>p -&gt; q</p>
<ul>
<li><p>p = beta is greater than one.</p></li>
<li><p>q = add one to the remainder of one minus beta.</p></li>
</ul>
</div>
<div id="unsupervised-thresholding." class="section level3">
<h3>Unsupervised thresholding.</h3>
<p>Finally, the image in <em>Figure 3</em> was constructed using the altered <span class="math inline">\(y\)</span> matrix of sigma values, which is contains the cut of point for unsupervised thresholding using the experimental technique proposed by Gavish &amp; Donoho, 2014, and alter ever so slightly by the author to keep the bounds of <span class="math inline">\(beta\)</span> within the range zero and one.</p>
<pre class="python"><code># Compute the SVD of Y.
U, S, VT = np.linalg.svd(Y,full_matrices=False)


# Get dimensions of the image and assign it to beta then check if greater than 1 
# else make 1 - remainder of 1 - beta. Create omega as per Gavish &amp; Donoho, 2014
# Set sigma to half the standard deviation of Y to compensate for Y = X + Noise.
beta = Y.shape[0] / Y.shape[1];
if beta &gt; 1:
    beta = 1 + (1 - beta)
omega = (0.56*beta**3) - (0.95*beta**2) + 1.82*beta + 1.43;   
sigma = np.std(Y)/2


# Print general output of omega, beta, sigma and std.
print(&quot;Beta: {:5.2f}&quot;.format(beta))
print(&quot;Omega: {:0.2f}&quot;.format(omega))
print(&quot;Sigma: {:0.2f}&quot;.format(sigma))
cutoff =  omega * np.median(S)


# Assign the Sigma values, S, from the np.linalg.svd to a variable y, which 
# enables the singular value of S to be retained, then set anything below 
# the cutoff point to zero, followed by a reconstruct of X in the form of Xclean.
y = S
y[y &lt; (cutoff)] = 0
N = Y.shape[0]
Xclean = U[:,:] @ np.diag(y) @ VT[:,:]


plt.imshow(Xclean)
plt.set_cmap(&#39;gray&#39;)
plt.axis(&#39;off&#39;)
plt.show()

print(&quot;This is standard deviation Y: {:12.2f}&quot;.format(np.std(Y)))
print(&quot;And the standard deviation of Xclean: {:0.2f}&quot;.format(np.std(Xclean)))</code></pre>
<table>
<thead>
<tr class="header">
<th>Gavish &amp; Donoho values</th>
<th>Image</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Beta</td>
<td>0.67</td>
</tr>
<tr class="even">
<td>Omega</td>
<td>2.39</td>
</tr>
<tr class="odd">
<td>Sigma</td>
<td>0.19</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th><img src="images/threshold_dog.png" /></th>
<th><img src="images/noisy_plus_dog.png" /></th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<center>
<em>Figure 3 - (Left) Image constructed using Gavin &amp; Donoho threshold &amp;</em><br>
</center>
<center>
<em>(Right) shows the image with Guassian noise.</em><br><br>
</center>
<table>
<thead>
<tr class="header">
<th>Standard deviation</th>
<th>Pre-Noise</th>
<th>Post-Noise</th>
<th>G &amp; D Thresholding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Output</td>
<td>0.27</td>
<td>0.38</td>
<td>0.27</td>
</tr>
</tbody>
</table>
<center>
<em>Table 2 - Comparision of noise.</em><br><br>
</center>
<p>At this point, it is best to refrain from comparing the quality of the output of the images in <em>Figure 3</em> as the whole point is to test the thresholding techniques. However, it is interesting to note, that unsupervised thresholding was able to match the standard deviation of the original grey scaled image (see <em>Table 2</em>).</p>
<p>Investigated next is a commonly used threshold technique in teaching the concepts of SVD; it works by adding the <span class="math inline">\(\sigma\)</span> values until it reaches 90%. The reconstructed image in <em>Figure 4</em> using the percentage of each component <span class="math inline">\(U\)</span>, <span class="math inline">\(S\)</span>, <span class="math inline">\(V^{T}\)</span>, is shown alongside the image reconstructed with Gavin &amp; Donoho thresholding.</p>
<pre class="python"><code>cdS = np.cumsum(S) / np.sum(S) # Cumulative energy
r90 = np.min(np.where(cdS &gt; 0.90)) # Find r to capture 90% energy

X90 = U[:,:(r90+1)] @ np.diag(S[:(r90+1)]) @ VT[:(r90+1),:]
plt.imshow(X90)
plt.set_cmap(&#39;gray&#39;)
plt.axis(&#39;off&#39;)
plt.show()
print(&quot;And the standard deviation of X90: {:0.2f}&quot;.format(np.std(X90)))</code></pre>
<table>
<thead>
<tr class="header">
<th><img src="images/Percent90_dog.png" /></th>
<th><img src="images/threshold_dog.png" /></th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<center>
<em>Figure 4 - (Left) Image constructed using 90% threshold &amp; </em><br>
</center>
<center>
<em>(Right) shows Image constructed using Gavin &amp; Donoho threshold.</em><br><br>
</center>
<table>
<thead>
<tr class="header">
<th>Standard deviation</th>
<th>Pre-Noise</th>
<th>Post-Noise</th>
<th>G &amp; D Thresholding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Output</td>
<td>0.27</td>
<td>0.38</td>
<td>0.27</td>
</tr>
</tbody>
</table>
</div>
<div id="examining-the-threshold." class="section level3">
<h3>Examining the threshold.</h3>
<p>So although the unsupervised thresholding does not eradicate the noise, it improves the clarity of the image seen on the right in <em>Figure 4</em>. Other more advanced techniques could improve the image quality. Nevertheless, this was not the point, as the aim was to find an accurate thresholding technique. The <em>Figure 5 &amp; 6</em> below show how accurate the algorithm by Gavin &amp; Donoho, 2014 in predicting the precise cut-off point, which, is the whole purpose of this article, finding an unsupervised threshold for the reconstruction of the Singular Value Decomposition.</p>
<pre class="python"><code># Change floating point to integer variables for indexing the matrices. r90 is already an 
# integer SVD of the 90% thresholding.  The shape of the image with noise is added to N.
int_cutoff = int(round(cutoff, 0))
N = Y.shape[0]


# Next the figure is plotted. The Reconstructed image is showing in black line
# with dotes marked at spaced intervals denoted by the &#39;k&#39; and &#39;o&#39;. The 90% 
# is presented as a blue line, &#39;b&#39;, with a diamond shaped marker &#39;D&#39;.  Finally,
# the G &amp; D threshold, int_cutoff, is showing using the dot maker, &#39;o&#39;, with a 
# red line, &#39;r&#39;.However for some reason the cutoff required an addition of 2, ie.
# int_cutoff+2, where as the ax1.plot only required the constant 1 added to 
# int_cutoff+1 which makes sense as Python indexes at zero not one. Have left the
# int_cutoff+2 as it correctly marks the true cut off point. Using plt.subplots
# is not necessary, yet original there were two plots and it does make adding
# further graphs easier.
fig1,ax1 = plt.subplots(1)
ax1.plot(cdS, marker=&#39;o&#39;, color=&#39;k&#39;, LineWidth=2)
ax1.plot(cdS[:(r90+1)], marker=&#39;D&#39;, color=&#39;b&#39;, markeredgewidth=3, LineWidth=2)
ax1.plot(cdS[:(int_cutoff+2)],&#39;o&#39;,color=&#39;r&#39;,LineWidth=2)
plt.xticks(np.array([0, 5, 10, 15, 20, 25, 30, 35]))
plt.xlim(0,35)
plt.ylim(0,1.5)


# This part is to make the dotted perpendicular lines, &#39;--&#39; that meets at the 
# cutoff point for 90% and G &amp; D thresholds and are coloured blue, &#39;b&#39; and red,
# &#39;r&#39;, respectively.
ax1.plot(np.array([r90, r90, -10]), np.array([0, 0.9, 0.9]), &#39;--&#39;, color=&#39;b&#39;, LineWidth=2)
ax1.plot(np.array([int_cutoff+1, int_cutoff+1, -10]), np.array([0, 1, 1]), &#39;--&#39;, color=&#39;r&#39;, LineWidth=2)
plt.legend([&#39;Reconstructed Image&#39;, &#39;90% Threshold&#39;, &#39;G &amp; D Threshold&#39;], loc=&#39;upper right&#39;)
ax1.grid()
plt.savefig(&quot;images/GDthresholdv90p.png&quot;)
plt.show()</code></pre>
<img src="images/GDthresholdv90p.png" />
<center>
<em>Figure 7 - Reconstructed image with 90% and G &amp; D thresholds.</em><br><br>
</center>
</div>
<div id="the-result." class="section level3">
<h3>The result.</h3>
<p>As it turns out the first 26 columns of the reconstructed components <span class="math inline">\(U\)</span>, <span class="math inline">\(S\)</span>, <span class="math inline">\(V^T\)</span> holds one hundred per cent of the information need to adequately reproduce the image, remembering that this is out of two thousand columns in each component! However, do not be fooled into thinking that plugging in one hundred per cent and get the right cut-off point. As there are times when the best result attained can be well below a hundred per cent before the information repeats itself and does not improve or provide anything new to the problem at hand. The outcome would be that the function would return the whole matrix of information because it can not reach one hundred or even nineteen per cent for that matter. In saying that, the article by Gavin &amp; Donoho, 2014 was both extensive and peer-reviewed, and therefore, the reader can be the algorithm does work.</p>
</div>
<div id="further-examination." class="section level3">
<h3>Further examination.</h3>
<p>On review, the author neglected to test the following logic on a large enough sample;</p>
<p>p -&gt; q</p>
<ul>
<li><p>p = beta is higher than one.</p></li>
<li><p>q = add one to the remainder of one minus beta.</p></li>
</ul>
<p>Therefore the next post aims to extract images from the <a href="&#39;https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip&#39;">cats and dog repository of images</a> that are greater than a thousand pixels wide to test the efficacy of the logic combine with the G &amp; D thresholding.</p>
</div>
<div id="references" class="section level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-gavish_optimal_2014">
<p>[1] M. Gavish and D. L. Donoho, “The optimal hard threshold for singular values is 4/sqrt(3),” <em>arXiv:1305.5870 [stat]</em>, Jun. 2014 [Online]. Available: <a href="http://arxiv.org/abs/1305.5870">http://arxiv.org/abs/1305.5870</a>. [Accessed: 13-Feb-2020]</p>
</div>
<div id="ref-brunton_data-driven_2019">
<p>[2] S. L. Brunton and J. N. Kutz, <em>Data-driven science and engineering machine learning, dynamical systems, and control</em>. Cambridge University Press, 2019. </p>
</div>
<div id="ref-takacs_distribution_1970">
<p>[3] L. Takács, “On the distribution of the maximum of sums of mutually independent and identically distributed random variables,” <em>Advances in Applied Probability</em>, vol. 2, no. 2, pp. 344–354, 1970 [Online]. Available: <a href="https://www.jstor.org/stable/1426323">https://www.jstor.org/stable/1426323</a>. [Accessed: 13-Feb-2020]</p>
</div>
</div>
</div>
